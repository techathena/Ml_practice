# -*- coding: utf-8 -*-
"""Experinmnet_1athena.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IXcWTR1OJH1paMdM66PCp-UEWLsQ7Qv3

# Read CSV Files:

A simple way to store big data sets is to use CSV files (comma separated files).

CSV files contains plain text and is a well know format that can be read by everyone including Pandas.

In our examples we will be using a CSV file called 'data.csv'.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv("/content/drive/My Drive/project videos/cancer.csv")
df

"""# Viewing the Data:

One of the most used method for getting a quick overview of the DataFrame, is the head() method.

The head() method returns the headers and a specified number of rows, starting from the top.
"""

df.head(10)

#to read the last end of data
df.tail()

"""# Info About the Data:

The DataFrames object has a method called info(), that gives you more information about the data set.
"""

df.info()

df.shape

#print all the columns of dataset
df.columns.values

"""# Finding Relationships:

A great aspect of the Pandas module is the corr() method.

The corr() method calculates the relationship between each column in your data set.


"""

df.corr()

#check for the null value
df.isnull().sum()

"""# Result Explained:

The Result of the corr() method is a table with a lot of numbers that represents how well the relationship is between two columns.

The number varies from -1 to 1.

1 means that there is a 1 to 1 relationship (a perfect correlation), and for this data set, each time a value went up in the first column, the other one went up as well.

0.9 is also a good relationship, and if you increase one value, the other will probably increase as well.

-0.9 would be just as good relationship as 0.9, but if you increase one value, the other will probably go down.

0.2 means NOT a good relationship, meaning that if one value goes up does not mean that the other will.

# Data Cleaning:

Data cleaning means fixing bad data in your data set.

Bad data could be:

-Empty cells

-Data in wrong format

-Wrong data

-Duplicates

# Empty Cells
Empty cells can potentially give you a wrong result when you analyze data.

# Remove Rows
One way to deal with empty cells is to remove rows that contain empty cells.

This is usually OK, since data sets can be very big, and removing a few rows will not have a big impact on the result.
"""

for i in df.columns:
    print(i)
    print(df[i].value_counts())
    print('-----------------------*********---------------------------')

df['diagnosis'].value_counts()

df= df.drop(["id"], axis = 1)
df

df = df.drop(["Unnamed: 32"], axis = 1)
df

"""# Visualization
it is import to see that counts of different type of cancer
"""

import matplotlib.pyplot as plt
import seaborn as sns

benign, malignant=df['diagnosis'].value_counts()
print("No of Benign cell", benign)
print("No of malignant cell", malignant)

plt.figure(figsize=(10,10))
sns.countplot(df['diagnosis'])
plt.show()

print("% of Benign cell is ", benign*100/len(df))
print("% of Malignant cell is ", malignant*100/len(df))

df.diagnosis.value_counts().plot(kind='pie',shadow=True,colors=('darkgreen','orange'),autopct='%.2f',figsize=(8,6))
plt.title('Diagnosis')
plt.show()

"""Pairplot helps to plot among the most useful feature"""

cols=['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']
plt.figure(figsize=(10,10))
sns.pairplot(data=df[cols],hue='diagnosis', palette='RdBu')

"""Heatmap:

To find the most correlated features
"""

import numpy as np

#generate the corelation matrix 
corr=df.corr().round(2)
#mask for the upper triangle
mask=np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)]
# Set figure size
f, ax = plt.subplots(figsize=(20, 20))

#define custom colormap
cmap=sns.diverging_palette(220,10, as_cmap=True)

#draw the heatmap
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)

plt.tight_layout()

# Generate and visualize the correlation matrix
corr = df.corr().round(2)

# Mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set figure size
f, ax = plt.subplots(figsize=(20, 20))

# Define custom colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)

plt.tight_layout()

M = df[df.diagnosis == "M"]
M.head()

B = df[df.diagnosis == "B"]
B.head()

plt.title("Malignant vs Benign Tumor")
plt.xlabel("Radius Mean")
plt.ylabel("Texture Mean")
plt.scatter(M.radius_mean, M.texture_mean, color = "red", label = "Malignant", alpha = 0.3)
plt.scatter(B.radius_mean, B.texture_mean, color = "lime", label = "Benign", alpha = 0.3)
plt.legend()
plt.show()

"""# Meaning Of Decision Tree Algorithm
Decision tree models where the target variable uses a discrete set of values are classified as Classification Trees.

In these trees, each node, or leaf, represent class labels while the branches represent conjunctions of features leading to class labels.

A decision tree where the target variable takes a continuous value, usually numbers, are called Regression Trees.

The two types are commonly referred to together at CART (Classification and Regression Tree).

![image.png](attachment:image.png)

# Decision Tree with Sklearn
"""

feature_cols = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

x = df[feature_cols]
y = df.diagnosis.values

x.head()

"""# What is normalization?

Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. Normalization is also required for some algorithms to model the data correctly.

## MinMax: 
The min-max normalizer linearly rescales every feature to the [0,1] interval.

Rescaling to the [0,1] interval is done by shifting the values of each feature so that the minimal value is 0, and then dividing by the new maximal value (which is the difference between the original maximal and minimal values).

The values in the column are transformed using the following formula:

normalization using the min-max function

![image.png](attachment:image.png)
"""

# Normalization:
x = (x - np.min(x)) / (np.max(x) - np.min(x))
x

from sklearn.model_selection import train_test_split

#for checking testing results
from sklearn.metrics import classification_report, confusion_matrix

#for visualizing tree 
from sklearn.tree import plot_tree

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

print("Training split input- ", x_train.shape)
print("Testing split input- ", x_test.shape)

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()

dt.fit(x_train, y_train)

"""### Testing

#### Precision — Also called Positive predictive value
The ratio of correct positive predictions to the total predicted positives.
#### Recall — Also called Sensitivity, Probability of Detection, True Positive Rate

The ratio of correct positive predictions to the total positives examples.

### Confusion matrix
 confusion matrix usage to evaluate the quality of the output of a classifier. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.

![image.png](attachment:image.png)

![image.png](attachment:image.png)

### Accuracy
Talking about accuracy, our favourite metric!

Accuracy is defined as the ratio of correctly predicted examples by the total examples.

![image.png](attachment:image.png)

![image-2.png](attachment:image-2.png)
"""

y_pred = dt.predict(x_test)
print("Classification report - \n", classification_report(y_test,y_pred))

cm=confusion_matrix(y_test,y_pred)
cm

plt.figure(figsize=(5,5))

sns.heatmap(data=cm,linewidths=1.0, annot=True,square = True,  cmap = 'Blues')

plt.ylabel('Actual label')
plt.xlabel('Predicted label')

all_sample_title = 'Accuracy Score: {0}'.format(dt.score(x_test, y_test))
plt.title(all_sample_title, size = 15)

plt.savefig("/content/drive/My Drive/project videos")

! pip install graphviz

# Visualising the graph without the use of graphviz

plt.figure(figsize = (50,50))
dec_tree = plot_tree(decision_tree=dt, feature_names = df.columns, class_names =["Malignant", "Benign"] , filled = True , precision = 4, rounded = True)

plt.savefig("/content/drive/My Drive/project videos")

